from backend.src.core.embeddings import EmbeddingsHandler
from backend.src.models.vector_store import VectorStore
from backend.src.models.chat_model import ChatModel
from backend.src.core.config import config
import logging

# Set up logging for better tracking of events
logging.basicConfig(level=logging.INFO)

class RAGSystem:
    def __init__(self, openai_api_key):
        """Initializes the RAG System with necessary components."""
        self.embeddings_handler = EmbeddingsHandler(openai_api_key)
        self.vector_store = VectorStore(openai_api_key)
        self.chat_model = ChatModel(openai_api_key)

    def add_documents(self, documents):
        """Adds documents to the vector store and creates embeddings."""
        try:
            self.vector_store.add_documents(documents)
        except Exception as e:
            logging.error(f"Error in adding documents: {str(e)}")
            raise

    def answer_query(self, query):
        """
        Answers a user's query using the chat model and vector store.

        Args:
            query (str): The user's query.

        Returns:
            str: The response generated by the chat model.
        """
        try:
            logging.info(f"Received query: {query}")
            
            # Use the vector store to retrieve relevant documents based on the query
            relevant_docs = self.vector_store.query(query)
            logging.info(f"Retrieved documents: {relevant_docs}")

            # Ensure the documents returned are in the expected format (list of strings or list of dicts with 'text' keys)
            if isinstance(relevant_docs, list) and isinstance(relevant_docs[0], str):
                relevant_texts = relevant_docs  # Directly use the texts if they're strings
            elif isinstance(relevant_docs, list) and isinstance(relevant_docs[0], dict):
                relevant_texts = [doc.get('text', '') for doc in relevant_docs]  # Extract text from dicts
            else:
                raise ValueError("The retrieved documents are neither strings nor dicts with 'text' keys.")

            # Use the chat model to generate an answer based on the retrieved documents
            chat_qa = self.chat_model.create_chat_qa(self.vector_store)
            response = chat_qa({"question": query, "chat_history": relevant_texts})

            return response['answer']
        except Exception as e:
            logging.error(f"Error in answering query: {str(e)}")
            raise
