# src/models/rag_system.py

from src.core.embeddings import EmbeddingsHandler
from src.models.vector_store import VectorStore
from src.models.chat_model import ChatModel
from src.core.config import config

class RAGSystem:
    def __init__(self, openai_api_key):
        """Initializes the RAG System with necessary components."""
        self.embeddings_handler = EmbeddingsHandler(openai_api_key)
        self.vector_store = VectorStore(openai_api_key)
        self.chat_model = ChatModel(openai_api_key)

    def add_documents(self, documents):
        """
        Adds documents to the vector store and creates embeddings.

        Args:
            documents (list): A list of documents to add to the store.
        """
        self.vector_store.add_documents(documents)

    def answer_query(self, query):
        """
        Answers a user's query using the chat model and vector store.

        Args:
            query (str): The user's query.

        Returns:
            str: The response generated by the chat model.
        """
        # Use the vector store to retrieve relevant documents based on the query
        relevant_docs = self.vector_store.query(query)

        # Use the chat model to generate an answer based on the retrieved documents
        chat_qa = self.chat_model.create_chat_qa(self.vector_store)
        response = chat_qa({"question": query, "chat_history": relevant_docs})

        return response['answer']

# Example usage
if __name__ == "__main__":
    rag_system = RAGSystem(openai_api_key=config.OPENAI_API_KEY)
    sample_documents = [
        "This is the first document about legal terms.",
        "Here is another document discussing contract clauses.",
        "This document explains the rights of the parties involved."
    ]

    rag_system.add_documents(sample_documents)

    user_query = "What are the rights of the parties involved in a contract?"
    response = rag_system.answer_query(user_query)
    print("Response:", response)
