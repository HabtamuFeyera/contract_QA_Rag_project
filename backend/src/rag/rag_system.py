import logging
from ..core.embeddings import EmbeddingsHandler
from ..models.vector_store import VectorStore
from ..models.chat_model import ChatModel
from ..core.config import config


logging.basicConfig(level=logging.INFO)

class RAGSystem:
    def __init__(self, openai_api_key):
        """
        Initializes the RAG System with necessary components.
        
        Args:
            openai_api_key (str): The API key for OpenAI services.
        """
        self.embeddings_handler = EmbeddingsHandler(openai_api_key)
        self.vector_store = VectorStore(openai_api_key)
        self.chat_model = ChatModel(openai_api_key)

    def add_documents(self, documents):
        """
        Adds documents to the vector store and creates embeddings.
        
        Args:
            documents (list): A list of documents to add to the vector store.
        """
        try:
            logging.info(f"Starting to add {len(documents)} documents to vector store.")
            self.vector_store.add_documents(documents)
            logging.info(f"Successfully added {len(documents)} documents to the vector store.")
        except Exception as e:
            logging.error(f"Error while adding documents to vector store: {str(e)}")
            raise

    def answer_query(self, query):
        """
        Answers a user's query using the chat model and vector store.
        
        Args:
            query (str): The user's query.
        
        Returns:
            str: The response generated by the chat model.
        """
        try:
            logging.info(f"Received query: {query}")
            
            # Retrieve relevant documents from the vector store based on the query
            relevant_docs = self.vector_store.query(query)
            logging.info(f"Retrieved {len(relevant_docs)} relevant documents.")

            # If no relevant documents found, return an informative message
            if not relevant_docs:
                logging.warning("No relevant documents found for the query.")
                return "Sorry, I couldn't find any relevant information. Please try rephrasing your query."

            
            relevant_texts = self._extract_relevant_texts(relevant_docs)

            # Use the chat model to generate an answer based on the retrieved documents
            chat_qa = self.chat_model.create_chat_qa(self.vector_store)
            response = chat_qa({"question": query, "chat_history": relevant_texts})

            # Validate and return the response from the chat model
            return self._validate_chat_response(response)

        except ValueError as e:
            logging.error(f"Error in query answer processing: {str(e)}")
            raise
        except Exception as e:
            logging.error(f"Unexpected error in answering query: {str(e)}")
            raise

    def _extract_relevant_texts(self, relevant_docs):
        """
        Extracts the relevant texts from the retrieved documents.
        
        Args:
            relevant_docs (list): The list of retrieved documents (strings or dicts).
        
        Returns:
            list: A list of relevant texts (strings).
        """
        if isinstance(relevant_docs, list):
            if isinstance(relevant_docs[0], str):
                return relevant_docs  # Return the texts directly if they are strings
            elif isinstance(relevant_docs[0], dict):
                return [doc.get('text', '') for doc in relevant_docs]  # Extract 'text' if documents are dicts
            else:
                raise ValueError("Retrieved documents are neither strings nor dictionaries with 'text' keys.")
        else:
            raise ValueError("Expected a list of documents but got something else.")
    
    def _validate_chat_response(self, response):
        """
        Validates the response from the chat model.
        
        Args:
            response (dict): The response from the chat model.
        
        Returns:
            str: The answer from the response.
        
        Raises:
            ValueError: If the response does not contain a valid 'answer' key.
        """
        if isinstance(response, dict) and 'answer' in response:
            return response['answer']
        else:
            logging.error(f"Received invalid response from chat model: {response}")
            raise ValueError("Invalid response from chat model. The response must contain an 'answer' key.")
